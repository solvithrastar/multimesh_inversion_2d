{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2D Smoothiesem inversion notebook\n",
    "\n",
    "This notebook goes through every step of the way to perform a synthetic 2D inversion, using Smoothiesem type of meshes.\n",
    "\n",
    "Unfortunately not all parts of the code can be made publicly available at this time at least so this version of the notebook will not be able to run. It does however show how things are done. If there are any issues with running this manuscript and reproducing the results, please contact the author as some of the libraries used may have changed by now.\n",
    "\n",
    "Initially packages are loaded and all used functions defined.\n",
    "The first iteration is then performed with some communication with Salvus Opt.\n",
    "After the first iteration is done, then by always naming the new iteration with the same names that Salvus Opt gives it, the rest of the inversion can be performed automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "\n",
    "import h5py\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import inspect\n",
    "import stat\n",
    "import shutil\n",
    "import pyasdf\n",
    "import pathlib\n",
    "import toml\n",
    "import math\n",
    "import obspy\n",
    "import copy\n",
    "import pyexodus\n",
    "\n",
    "import salvus_flow\n",
    "from salvus_flow import api\n",
    "from obspy.signal import filter\n",
    "\n",
    "from salvus_mesh.structured_grid_2D import StructuredGrid2D\n",
    "from salvus_mesh.skeleton import Skeleton\n",
    "import salvus_misfit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create directory structure and save paths\n",
    "\n",
    "We want a directory tree like this for our projects:\n",
    "\n",
    "                                                    Inversion\n",
    "                                                        |\n",
    "            ------------------------------------------------------------------------------------------------------\n",
    "            |                |                  |                      |                        |                |\n",
    "       INFORMATION     ADJOINT_SOURCES      SYNTHETICS               MESHES                    DATA          GRADIENT\n",
    "            |                |                  |                      |                        |\n",
    "        ITERATION        ITERATION          ITERATION             -----------                 SOURCE\n",
    "            |                |                  |                 |         |                   |\n",
    "          SOURCE          ASDF Files        ASDF Files        ITERATION  True model          True data\n",
    "            |                                                     |\n",
    "      ---------------                                         Smoothies\n",
    "      |             |\n",
    "    Misfit   Simulation I/O\n",
    "    \n",
    "    \n",
    "#### Define a few classes used and then some functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a few paths\n",
    "class Path:\n",
    "    def __init__(self, name):\n",
    "        self.root = \"/Drive/Users/solvi/workspace/synthetic_inversion/smoothiesem2D/\" + name\n",
    "        self.information = os.path.join(self.root, \"INFORMATION\")\n",
    "        self.adjoint_sources = os.path.join(self.root, \"ADJOINT_SOURCES\")\n",
    "        self.synthetics = os.path.join(self.root, \"SYNTHETICS\")\n",
    "        self.meshes = os.path.join(self.root, \"MESHES\")\n",
    "        self.data = os.path.join(self.root, \"DATA\")\n",
    "        self.gradients = os.path.join(self.root, \"GRADIENTS\")\n",
    "        self.salvusopt = os.path.join(self.root, \"SALVUSOPT\")\n",
    "    \n",
    "    def create_directory_structure(self):\n",
    "        if os.path.exists(self.root):\n",
    "            raise Error(f\"Path {self.root} already exists\")\n",
    "        # Create a directory structure for project\n",
    "        os.mkdir(self.root)\n",
    "        os.mkdir(self.information)\n",
    "        os.mkdir(self.adjoint_sources)\n",
    "        os.mkdir(self.synthetics)\n",
    "        os.mkdir(self.meshes)\n",
    "        os.mkdir(self.data)\n",
    "        os.mkdir(self.gradients)\n",
    "        os.mkdir(self.salvusopt)\n",
    "        os.mkdir(os.path.join(self.root, \"SHELL_RUNS\"))\n",
    "        os.mkdir(os.path.join(self.root, \"OUTPUT\"))\n",
    "        print(f\"A new project was created in path: {self.root}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Parameters:\n",
    "    def __init__(self, fmax, start_time, end_time, time_step, abs_layers):\n",
    "        self.fmax = fmax\n",
    "        self.start_time = start_time\n",
    "        self.end_time = end_time\n",
    "        self.time_step = time_step\n",
    "        self.abs_layers = abs_layers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Source:\n",
    "    def __init__(self, name, loc_x, loc_y, scale):\n",
    "        self.name = name\n",
    "        self.loc_x = loc_x * 1000  # Input in km but stored in meters.\n",
    "        self.loc_y = loc_y * 1000\n",
    "        self.scale = scale\n",
    "        self.rec_info_dict = {}\n",
    "    \n",
    "    def add_receiver(self, network, name, loc_x, loc_y):\n",
    "        self.rec_info_dict[name] = {\"loc_x\": loc_x * 1000, \"loc_y\": loc_y * 1000, \"network\": network}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_project(name):\n",
    "    # Make an instance of the Path class\n",
    "    paths = Path(name)\n",
    "    paths.create_directory_structure()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### To make a regular grid mesh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cartesian_mesh(vp, vs, rho, fmax=0.2,\n",
    "                  elements_per_wavelength=2., minx=0, maxx=0, miny=1000, maxy=1000, plot=False):\n",
    "    \"\"\"\n",
    "    build rectangular mesh\n",
    "    \"\"\"\n",
    "    hmax = vs / fmax / elements_per_wavelength\n",
    "    nelem_x = int(np.ceil((maxx-minx) / hmax))\n",
    "    nelem_y = int(np.ceil((maxy-miny) / hmax))\n",
    "    \n",
    "    # create box mesh without refinements\n",
    "    sg = StructuredGrid2D.rectangle(\n",
    "        nelem_x,nelem_y, min_x=minx, max_x=maxx, min_y=miny, max_y=maxy)\n",
    "\n",
    "    m = sg.get_unstructured_mesh()\n",
    "\n",
    "    m.find_side_sets(mode='cartesian')\n",
    "    attach_homogeneous_model(m, vp, vs, rho)\n",
    "    \n",
    "    if plot:\n",
    "        m.plot()\n",
    "\n",
    "    return m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Input file generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input file generation\n",
    "\n",
    "def generate_receivers_toml(tomlFileName, info):\n",
    "\n",
    "    rstr = '[[receiver]]\\nnetwork = \"RG\"\\nstation = \"%s\"\\nlocation = \"\"\\n' + \\\n",
    "           'medium = \"solid\"\\n' + \\\n",
    "           '\\nsalvus_coordinates = [%f, %f]\\n\\n'\n",
    "\n",
    "    with open(tomlFileName, 'w') as f:\n",
    "        for rec in info:\n",
    "            f.write(rstr % (rec, info[rec][\"loc_x\"], info[rec][\"loc_y\"]))\n",
    "        \n",
    "def generate_source_toml(tomlFileName, name, locx, locy, freq, scale):\n",
    "\n",
    "    sstr = '[[source]]\\nname = \"%s\"\\n' + \\\n",
    "           'temporal_type = \"ricker\"\\n' + \\\n",
    "           'spatial_type = \"moment_tensor\"\\n' + \\\n",
    "            'location = [%f, %f]\\n' + \\\n",
    "            'center_frequency = %f\\n' + \\\n",
    "            'scale = [%f, %f, %f]\\n'\n",
    "            \n",
    "    with open(tomlFileName, 'w') as f:\n",
    "        f.write(sstr % (name, locx, locy, freq, scale[0], scale[1], scale[2])) \n",
    "\n",
    "def generate_input_files(source, iteration, sim_type, sim_params): # Remember to create iteration first.\n",
    "    if sim_type == \"forward\":\n",
    "        generate_input_files_forward(source, iteration, sim_params)\n",
    "    elif sim_type == \"adjoint\":\n",
    "        generate_input_files_adjoint(source, iteration, sim_params)\n",
    "    else:\n",
    "        raise ValueError(\"Only simulation types forward and adjoint available\")\n",
    "\n",
    "def generate_input_files_forward(source, iteration, sim_params):  \n",
    "    input_file_direc = os.path.join(paths.information, iteration, source.name, \"forward\")\n",
    "    mesh_direc = os.path.join(paths.meshes, iteration, source.name) # Smoothie\n",
    "#     mesh_direc = os.path.join(paths.meshes, iteration) # Cartesian\n",
    "    if not os.path.exists(input_file_direc):\n",
    "        os.mkdir(input_file_direc)\n",
    "    runfile_name = os.path.join(input_file_direc, \"run_salvus_smoothie.sh\")\n",
    "    src_toml = os.path.join(input_file_direc, \"source.toml\")\n",
    "    rec_toml =  os.path.join(input_file_direc, \"receivers.toml\")\n",
    "    mesh_file = os.path.join(mesh_direc, \"smoothiesem_2d.e\")\n",
    "    rec_file_name = os.path.join(input_file_direc, \"receivers.h5\")\n",
    "    static_file_name = os.path.join(mesh_direc, \"smoothiesem_2d.h5\")\n",
    "    wavefield_file = os.path.join(input_file_direc, \"fwd_smoothiesem_2d.h5\")\n",
    "    \n",
    "    generate_source_toml(tomlFileName=src_toml, name=source.name, locx=source.loc_x, locy=source.loc_y,\n",
    "                        freq=fmax/2.0, scale=source.scale)\n",
    "    generate_receivers_toml(tomlFileName=rec_toml, info=source.rec_info_dict)\n",
    "    \n",
    "    run_str = MPI_BIN + \" -n 16 \" + SALVUS_BIN\n",
    "    run_str += \" --dimension 2 \" + \"--polynomial-order 4 \" \n",
    "    run_str += \"--mesh-file \" + mesh_file + \" \" + \"--model-file \" \n",
    "    run_str += mesh_file + \" \" + \"--load-static-fields model \" \n",
    "    run_str += \"--load-static-file-name \" + static_file_name + \" \" + \"--start-time \" \n",
    "    run_str += str(sim_params.start_time) + \" \" + \"--time-step \" + str(sim_params.time_step) + \" \" \n",
    "    run_str += \"--end-time \" + str(sim_params.end_time) + \" \" + \"--source-toml \" + src_toml + \" \" \n",
    "    run_str += \"--absorbing-boundaries surface \" + \"--num-absorbing-layers \" + str(sim_params.abs_layers) + \" \" \n",
    "    run_str += \"--receiver-toml \" + rec_toml + \" \" + \"--receiver-fields u_ELASTIC \" \n",
    "    run_str += \"--receiver-file-name \" + rec_file_name + \" \" + \"--save-fields adjoint \"\n",
    "    run_str += \"--save-wavefield-file \" + wavefield_file + \" \" + \"--io-sampling-rate-volume 20\"\n",
    "    \n",
    "    with open(runfile_name, \"w\") as f:\n",
    "        f.write(run_str)\n",
    "\n",
    "def generate_input_files_adjoint(source, iteration, sim_params):\n",
    "    input_file_direc = os.path.join(paths.information, iteration, source.name, \"adjoint\")\n",
    "    mesh_direc = os.path.join(paths.meshes, iteration, source.name) # Smoothie\n",
    "#     mesh_direc = os.path.join(paths.meshes, iteration) # Cartesian\n",
    "    adjoint_direc = os.path.join(paths.adjoint_sources, iteration, source.name)\n",
    "    if not os.path.exists(input_file_direc):\n",
    "        os.mkdir(input_file_direc)\n",
    "    runfile_name = os.path.join(input_file_direc, \"run_salvus_smoothie.sh\")\n",
    "    src_toml = os.path.join(adjoint_direc, \"adjoint.toml\")\n",
    "    mesh_file = os.path.join(mesh_direc, \"smoothiesem_2d.e\")\n",
    "    static_file_name = os.path.join(mesh_direc, \"smoothiesem_2d.h5\")\n",
    "    fwd_wavefield_file = os.path.join(paths.information, iteration, source.name, \"forward\", \"fwd_smoothiesem_2d.h5\")\n",
    "    adj_wavefield_file = os.path.join(input_file_direc, \"adj_smoothie.h5\")\n",
    "    kernel_file = os.path.join(input_file_direc, \"kernel.e\")\n",
    "    gradient_name = os.path.join(input_file_direc, \"gradient_gll.h5\")\n",
    "    \n",
    "    run_str = MPI_BIN + \" -n 16 \" + SALVUS_BIN\n",
    "    run_str += \" --dimension 2 \" + \"--polynomial-order 4 \" \n",
    "    run_str += \"--mesh-file \" + mesh_file + \" \" + \"--model-file \" \n",
    "    run_str += mesh_file + \" \" + \"--load-static-fields model \" \n",
    "    run_str += \"--load-static-file-name \" + static_file_name + \" \" + \"--start-time \" \n",
    "    run_str += str(sim_params.start_time) + \" \" + \"--time-step \" + str(sim_params.time_step) + \" \" \n",
    "    run_str += \"--end-time \" + str(sim_params.end_time) + \" \" + \"--source-toml \" + src_toml + \" \" \n",
    "    run_str += \"--adjoint \" + \"--kernel-file \" + kernel_file + \" --load-fields adjoint \"\n",
    "    run_str += \"--load-wavefield-file \" + fwd_wavefield_file + \" --save-static-fields gradient \"\n",
    "    run_str += \"--save-static-file-name \" + gradient_name + \" --kernel-fields MU,LAMBDA \"\n",
    "    run_str += \"--absorbing-boundaries surface \" + \"--num-absorbing-layers \" + str(sim_params.abs_layers) + \" \"\n",
    "    run_str += \"--save-wavefield-file \" + adj_wavefield_file + \" --save-fields u_ELASTIC \"\n",
    "    run_str += \"--io-sampling-rate-volume 20\"\n",
    "    \n",
    "    with open(runfile_name, \"w\") as f:\n",
    "        f.write(run_str)\n",
    "\n",
    "def make_gll_model_from_exodus(iteration, source_list=None, mesh_type=\"smoothiesem\"):\n",
    "    runfile_name = os.path.join(paths.meshes, iteration, \"run_salvus_make_gll.sh\")\n",
    "    with open(runfile_name, \"w\") as f:\n",
    "        if not source_list:\n",
    "            source_list = [0]\n",
    "            if mesh_type == \"smoothiesem\":\n",
    "                raise Error(\"smoothiesem mesh_type needs a list of sources\")\n",
    "        for source in source_list:\n",
    "            if mesh_type == \"cartesian\":\n",
    "                mesh_file = os.path.join(paths.meshes, iteration, \"cartesian_2d.e\")\n",
    "                h5model = os.path.join(paths.meshes, iteration, \"cartesian_2d.h5\")\n",
    "            if mesh_type == \"smoothiesem\":\n",
    "                mesh_file = os.path.join(paths.meshes, iteration, source.name, \"smoothiesem_2d.e\")\n",
    "                h5model = os.path.join(paths.meshes, iteration, source.name, \"smoothiesem_2d.h5\")\n",
    "            run_str = MPI_BIN + \" -n 16 \" + SALVUS_BIN\n",
    "            run_str += f\" --mesh-file {mesh_file} --model-file {mesh_file} \"\n",
    "            run_str += f\"--dimension 2 \" + \"--polynomial-order 4 \"\n",
    "            run_str += f\"--end-time 1 \" + \"--save-static-fields model \"\n",
    "            run_str += f\"--save-static-file-name {h5model} \\n\"\n",
    "\n",
    "            f.write(run_str)\n",
    "    return runfile_name\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def retrieve_synthetics(source_name, iteration):\n",
    "    # Find output from salvus and move to the correct directory to prepare for adjoint source calculation\n",
    "    salvus_output_path = os.path.join(paths.information, iteration, source_name, \"forward\")\n",
    "    waveforms = os.path.join(salvus_output_path, \"receivers.h5\")\n",
    "    synthetics_path = os.path.join(paths.synthetics, iteration, source_name, \"receivers.h5\")\n",
    "    data_path = os.path.join(paths.data, source_name, \"receivers.h5\")\n",
    "    shutil.copy(waveforms, synthetics_path)\n",
    "    \n",
    "def retrieve_gradient(source_name, iteration):\n",
    "    # Currently moves both exodus and hdf5 versions of the gradient.\n",
    "    salvus_adjoint_output_path = os.path.join(paths.information, iteration, source_name, \"adjoint\")\n",
    "    gradient = os.path.join(salvus_adjoint_output_path, \"kernel.e\")\n",
    "    gradient_gll = os.path.join(salvus_adjoint_output_path, \"gradient_gll.h5\")\n",
    "    gradient_gll_xdmf = os.path.join(salvus_adjoint_output_path, \"gradient_gll_ELASTIC.xdmf\")\n",
    "    shutil.copy(gradient, os.path.join(paths.gradients, iteration, source_name, \"kernel.e\"))\n",
    "    shutil.copy(gradient_gll, os.path.join(paths.gradients, iteration, source_name, \"gradient_gll.h5\"))\n",
    "    shutil.copy(gradient_gll_xdmf, os.path.join(paths.gradients, iteration, source_name, \"gradient_gll_ELASTIC.xdmf\"))\n",
    "\n",
    "def create_iteration(iteration_name):\n",
    "    # Create folder structure for iteration so we should never run into problems with directories which \n",
    "    # do not exist.\n",
    "    os.mkdir(os.path.join(paths.gradients, iteration_name))\n",
    "    os.mkdir(os.path.join(paths.information, iteration_name))\n",
    "    os.mkdir(os.path.join(paths.synthetics, iteration_name))\n",
    "    os.mkdir(os.path.join(paths.meshes, iteration_name))\n",
    "    if not os.path.exists(os.path.join(paths.root, \"OUTPUT\")):\n",
    "        os.mkdir(os.path.join(paths.root, \"OUTPUT\"))\n",
    "    os.mkdir(os.path.join(paths.root, \"OUTPUT\", iteration_name))\n",
    "    os.mkdir(os.path.join(paths.adjoint_sources, iteration_name))\n",
    "    for src in src_list:\n",
    "        os.mkdir(os.path.join(paths.gradients, iteration_name, src.name))\n",
    "        os.mkdir(os.path.join(paths.information, iteration_name, src.name))\n",
    "        os.mkdir(os.path.join(paths.information, iteration_name, src.name, \"forward\"))\n",
    "        os.mkdir(os.path.join(paths.information, iteration_name, src.name, \"adjoint\"))\n",
    "        os.mkdir(os.path.join(paths.synthetics, iteration_name, src.name))\n",
    "        os.mkdir(os.path.join(paths.meshes, iteration_name, src.name))\n",
    "        os.mkdir(os.path.join(paths.root, \"OUTPUT\", iteration_name, src.name))\n",
    "        os.mkdir(os.path.join(paths.adjoint_sources, iteration_name, src.name))\n",
    "\n",
    "def get_paths_to_waveforms(iteration_name, source_name):\n",
    "    synthetic = os.path.join(paths.synthetics, iteration_name, source_name, \"receivers.h5\")\n",
    "    observed = os.path.join(paths.data, source_name, \"receivers.h5\")\n",
    "    return synthetic, observed\n",
    "\n",
    "def move_meshes_between_iterations(previous_iteration, new_iteration, source_name):\n",
    "    old_mesh = os.path.join(paths.meshes, previous_iteration , source_name, \"smoothiesem_2d.e\")\n",
    "    old_model = os.path.join(paths.meshes, previous_iteration, source_name, \"smoothiesem_2d.h5\")\n",
    "    dest_mesh = os.path.join(paths.meshes, new_iteration, source_name, \"smoothiesem_2d.e\")\n",
    "    dest_model = os.path.join(paths.meshes, new_iteration, source_name, \"smoothiesem_2d.h5\")\n",
    "    shutil.copy(old_mesh, dest_mesh)\n",
    "    shutil.copy(old_model, dest_model)\n",
    "\n",
    "def distribute_smoothers(iteration):\n",
    "    smooth_script = os.path.join(paths.gradients, \"smooth_script.sh\")\n",
    "    dest_smoothie = os.path.join(paths.gradients, iteration, \"smooth_script.sh\")\n",
    "    shutil.copy(smooth_script, dest_smoothie)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adjoint Source Calculations and Misfit Quantification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_adjoint_sources(iteration, source, sim_params):\n",
    "    synthetic_waveform, observed_waveform = get_paths_to_waveforms(iteration, source.name)\n",
    "    \n",
    "    ad_src_type = \"waveform_misfit\"\n",
    "    min_period = 1.0 / sim_params.fmax - 1.0  \n",
    "    max_period = 1.0 / sim_params.fmax + 1.0\n",
    "    components = ['XDX', 'XDY']\n",
    "\n",
    "    with pyasdf.ASDFDataSet(synthetic_waveform, mode=\"a\") as syn:\n",
    "        with pyasdf.ASDFDataSet(observed_waveform) as obs:\n",
    "            for receiver in source.rec_info_dict:\n",
    "                rec = source.rec_info_dict[receiver][\"network\"] + \"_\" + receiver\n",
    "                s=0\n",
    "                if len(syn.auxiliary_data.list()) != 0:\n",
    "                    if rec in syn.auxiliary_data[\"AdjointSources\"]:\n",
    "                        del syn.auxiliary_data[\"AdjointSources\"][rec]\n",
    "                for component in components:\n",
    "                    rec = source.rec_info_dict[receiver][\"network\"] + \"_\" + receiver\n",
    "                    env = filter.envelope(obs.waveforms[rec].displacement[s].data)\n",
    "                    start_window_idx = np.where(np.abs(env) > 0.02 * np.amax(env))[0][0] - 120\n",
    "                    end_window_idx = np.where(np.abs(env) > 0.02 * np.amax(env))[0][-1] + 120\n",
    "                    adj_start_time = syn.waveforms[rec].displacement[s].stats.starttime\n",
    "                    adj_end_time = syn.waveforms[rec].displacement[s].stats.endtime\n",
    "                    adj_time_step = syn.waveforms[rec].displacement[s].stats.delta\n",
    "                    start_window = adj_start_time + adj_time_step * start_window_idx - sim_params.start_time\n",
    "                    end_window = adj_start_time + adj_time_step * end_window_idx - sim_params.start_time\n",
    "                    window = (start_window, end_window)\n",
    "                    observed = obspy.Trace(data=obs.waveforms[rec].displacement[s].data, \n",
    "                                           header=obs.waveforms[rec].displacement[s].stats)\n",
    "                    observed = copy.deepcopy(observed)\n",
    "                    synthetic = obspy.Trace(data=syn.waveforms[rec].displacement[s].data, \n",
    "                                            header=syn.waveforms[rec].displacement[s].stats)\n",
    "                    synthetic = copy.deepcopy(synthetic)\n",
    "                    adjoint_source = salvus_misfit.calculate_adjoint_source(ad_src_type, observed=observed, \n",
    "                                                                        synthetic=synthetic, min_period=min_period, \n",
    "                                                                        max_period=max_period, window=window, \n",
    "                                                                        plot=False)\n",
    "                    \n",
    "                    if adjoint_source.adjoint_source is None:\n",
    "                        s += 1\n",
    "                        continue\n",
    "                    if adjoint_source.adjoint_source is not None:\n",
    "                        ad_src = adjoint_source.adjoint_source\n",
    "                        ad_src = ad_src[::-1]\n",
    "                        misfit =  adjoint_source.misfit\n",
    "                        path = rec + \"/\" + component\n",
    "                        \n",
    "                        syn.add_auxiliary_data(data=ad_src, data_type=\"AdjointSources\", path=path, \n",
    "                                               parameters={\"misfit\": misfit})\n",
    "                        s += 1\n",
    "                    \n",
    "\n",
    "def write_adjoint_source_to_file(iteration, source, sim_params):\n",
    "    import toml\n",
    "    import h5py\n",
    "    synthetic_waveform, _ = get_paths_to_waveforms(iteration, source.name)\n",
    "    \n",
    "    ds = pyasdf.ASDFDataSet(synthetic_waveform)\n",
    "    adj_srcs = ds.auxiliary_data[\"AdjointSources\"]\n",
    "    output_dir = os.path.join(paths.adjoint_sources, iteration, source.name)\n",
    "    receiver_dir = os.path.join(paths.information, iteration, source.name, \"forward\")\n",
    "    receivers = toml.load(os.path.join(receiver_dir, \"receivers.toml\"))[\"receiver\"]\n",
    "    adjoint_source_file_name = os.path.join(output_dir, \"adjoint_source.h5\")\n",
    "    toml_file_name = os.path.join(output_dir, \"adjoint.toml\")\n",
    "    \n",
    "    toml_string = f\"source_input_file = \\\"{adjoint_source_file_name}\\\"\\n\\n\"\n",
    "    with h5py.File(adjoint_source_file_name, 'a') as f:\n",
    "    \n",
    "        for receiver in adj_srcs.list():\n",
    "            for channel in adj_srcs[receiver]:\n",
    "                # Need to do this for what happens later\n",
    "                x_comp = np.zeros(len(adj_srcs[receiver][channel.path].data.value))\n",
    "                y_comp = np.zeros(len(adj_srcs[receiver][channel.path].data.value))\n",
    "                continue\n",
    "            for channel in adj_srcs[receiver]: \n",
    "                if channel.path[-1] == \"X\":\n",
    "                    x_comp = adj_srcs[receiver][channel.path].data.value\n",
    "                elif channel.path[-1] == \"Y\":\n",
    "                    y_comp = adj_srcs[receiver][channel.path].data.value\n",
    "                else:\n",
    "                    print(\"Hey! Component neither X nor Y!\")\n",
    "            xy = np.array((x_comp, y_comp)).T\n",
    "            if receiver in f:\n",
    "                del f[receiver]\n",
    "            src = f.create_dataset(receiver, data=xy)\n",
    "            src.attrs[\"dt\"] = sim_params.time_step\n",
    "            src.attrs[\"location\"] = np.array([source.rec_info_dict[receiver[-4:]][\"loc_x\"], \n",
    "                                     source.rec_info_dict[receiver[-4:]][\"loc_y\"]], dtype=np.float64)\n",
    "            src.attrs[\"spatial-type\"] = np.string_(\"vector\")\n",
    "            src.attrs['starttime'] = sim_params.start_time * 1.0e9\n",
    "\n",
    "            toml_string += f\"[[source]]\\n\"\n",
    "            toml_string += f\"name = \\\"{receiver}\\\"\\n\"\n",
    "            toml_string += f\"dataset_name = \\\"/{receiver}\\\"\\n\\n\"\n",
    "\n",
    "    with open(toml_file_name, \"w\") as fh:\n",
    "        fh.write(toml_string)\n",
    "        amount = len(adj_srcs.list())\n",
    "        print(f\"Wrote {amount} adjoint sources to file\")\n",
    "        \n",
    "        \n",
    "def get_misfit_for_source(iteration, source):\n",
    "    \n",
    "    synthetic_waveform, _ = get_paths_to_waveforms(iteration, source.name)\n",
    "    \n",
    "    with pyasdf.ASDFDataSet(synthetic_waveform, mode=\"r\") as ds:\n",
    "        adj_src_data = ds.auxiliary_data[\"AdjointSources\"]\n",
    "        stations = ds.auxiliary_data[\"AdjointSources\"].list()\n",
    "        total_misfit = 0.0\n",
    "        for station in stations:\n",
    "            channels = adj_src_data[station].list()\n",
    "            for channel in channels:\n",
    "                misfit = adj_src_data[station][channel].parameters[\"misfit\"]\n",
    "                total_misfit += misfit\n",
    "                \n",
    "    return total_misfit\n",
    "    \n",
    "        \n",
    "def write_misfit_to_file(iteration, src_list):\n",
    "    import toml\n",
    "    misfit = 0.0\n",
    "    toml_dict = {}\n",
    "    toml_dict[\"misfits\"] = {}\n",
    "    toml_dict[\"misfits\"][\"source\"] = {}\n",
    "    toml_dict[\"misfits\"][\"total\"] = {}\n",
    "    toml_file_name = os.path.join(paths.adjoint_sources, iteration, \"misfits.toml\")\n",
    "\n",
    "    for src in src_list:\n",
    "        print(src.name)\n",
    "        event_misfit = get_misfit_for_source(iteration, src)\n",
    "        toml_dict[\"misfits\"][\"source\"][src.name] = event_misfit\n",
    "        misfit += event_misfit\n",
    "    toml_dict[\"misfits\"][\"total\"][\"total_misfit\"] = misfit\n",
    "    \n",
    "    with open(toml_file_name, \"w\") as fh:\n",
    "        toml.dump(toml_dict, fh)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preparation of shell scripts to run salvus or smoother"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_runfile(iteration_name, source_list, sim_type):\n",
    "    file_name = \"run_\" + sim_type + \"_simulations_\" + iteration_name + \".sh\"\n",
    "    file = os.path.join(paths.root, \"SHELL_RUNS\", file_name)\n",
    "    with open(file, \"w\") as fh:\n",
    "        for source in source_list:\n",
    "            run_file = os.path.join(paths.information, iteration_name, source.name, sim_type, \"run_salvus_smoothie.sh\")\n",
    "            outfile = os.path.join(paths.root, \"OUTPUT\", iteration_name, source.name, sim_type + \".txt\")\n",
    "            fh.write(\"sh \" + run_file + \" | tee \" + outfile + \" \\n\")\n",
    "    run_command = \"sh \" + file\n",
    "    return run_command\n",
    "\n",
    "def run_salvus_opt():\n",
    "    file_name = \"run_salvus_opt.sh\"\n",
    "    file = os.path.join(paths.root, \"SHELL_RUNS\", file_name)\n",
    "    with open(file, \"w\") as fh:\n",
    "        run_file = os.path.join(paths.salvusopt, \"run_salvus_opt.sh\")\n",
    "        fh.write(\"cd \" + paths.salvusopt + \" \\n\")\n",
    "        fh.write(\"sh \" + run_file + \"\\n\")\n",
    "        fh.write(\"cd -\")\n",
    "    run_command = \"sh \" + file\n",
    "    return run_command\n",
    "\n",
    "def smooth_gradient(iteration):\n",
    "    file_name = \"run_smoother_\" + iteration + \".sh\"\n",
    "    file = os.path.join(paths.root, \"SHELL_RUNS\", file_name)\n",
    "    with open(file, \"w\") as fh:\n",
    "        \n",
    "            smooth_script = os.path.join(paths.gradients, iteration, \"smooth_script.sh\")\n",
    "            grad_dir = os.path.join(paths.gradients, iteration)\n",
    "            fh.write(f\"cd {grad_dir} \\n\")\n",
    "            if not os.path.exists(os.path.join(grad_dir, \"OUT\")):\n",
    "                fh.write(\"mkdir OUT \\n\")\n",
    "            fh.write(\"sh \" + smooth_script + \"\\n\")\n",
    "    run_command = \"sh \" + file\n",
    "    return run_command"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Interpolation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multi_mesh.api as mapi\n",
    "\n",
    "def interpolate_from_cartesian_to_smoothie(iteration, src_name):\n",
    "    cartesian = os.path.join(paths.meshes, iteration, \"cartesian_2d.h5\")\n",
    "    smoothie = os.path.join(paths.meshes, iteration, src_name, \"smoothiesem_2d.h5\")\n",
    "    print(f\"Interpolating velocities for iteration: {iteration}, source: {src_name}\")\n",
    "    mapi.gll_2_gll(cartesian=cartesian, smoothie=smoothie)\n",
    "\n",
    "def interpolate_from_cartesian_to_smoothie_exodus(iteration, src_name):\n",
    "    cartesian = os.path.join(paths.meshes, iteration, \"cartesian_2d.h5\")\n",
    "    smoothie = os.path.join(paths.meshes, iteration, src_name, \"smoothiesem_2d.e\")\n",
    "    print(f\"Interpolating velocities for iteration: {iteration}, source: {src_name}\")\n",
    "    mapi.gll_2_exodus(cartesian=cartesian, smoothie=smoothie)\n",
    "\n",
    "def interpolate_gradient_to_cartesian(iteration, src_list):\n",
    "    i = 0\n",
    "    cartesian = os.path.join(paths.gradients, iteration, \"cartesian_kernel.e\")\n",
    "    for src in src_list:\n",
    "        print(f\"Interpolating gradient from source: {src.name}\")\n",
    "        smoothie_kern = os.path.join(paths.gradients, iteration, src.name, \"gradient_gll.h5\")\n",
    "        if i == 0:\n",
    "            mapi.gradient_2_cartesian_hdf5(gradient=smoothie_kern, cartesian=cartesian, first=True)\n",
    "        else:\n",
    "            mapi.gradient_2_cartesian_hdf5(gradient=smoothie_kern, cartesian=cartesian, first=False)\n",
    "        i += 1\n",
    "        \n",
    "def interpolate_smoothie_to_cartesian_hdf5(iteration, src_list):\n",
    "    i = 0\n",
    "    cartesian = os.path.join(paths.gradients, iteration, \"cartesian_kernel.h5\")\n",
    "    for src in src_list:\n",
    "        print(f\"Interpolating gradient from source: {src.name}\")\n",
    "        smoothie_kern = os.path.join(paths.gradients, iteration, src.name, \"gradient_gll.h5\")\n",
    "        if i == 0:\n",
    "            mapi.gll_2_gll_gradients(smoothie_kern, cartesian, first=True)\n",
    "        else:\n",
    "            mapi.gll_2_gll_gradients(smoothie_kern, cartesian, first=False)\n",
    "        i += 1\n",
    "\n",
    "        \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cut source and receivers from gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial import cKDTree\n",
    "def cut_receiver_from_gradient(source, iteration, radius_in_km):\n",
    "    # Cut a small area around the receiver locations from the gradient\n",
    "    # Loop through receivers that record the source and find which gll points are close\n",
    "    # put gradient values to zero on these points.\n",
    "    gradient = os.path.join(paths.gradients, iteration, source.name, \"gradient_gll.h5\")\n",
    "    orig = os.path.join(paths.gradients, iteration, source.name, \"original_gradient.h5\")\n",
    "    if not os.path.exists(orig):\n",
    "        shutil.copy(gradient, orig)\n",
    "    else:\n",
    "        shutil.copy(orig, gradient)\n",
    "    a = 0\n",
    "    receivers = np.zeros([len(source.rec_info_dict),2])\n",
    "    for rec in source.rec_info_dict:\n",
    "        rec_x = source.rec_info_dict[rec][\"loc_x\"]\n",
    "        rec_y = source.rec_info_dict[rec][\"loc_y\"]\n",
    "        \n",
    "        receivers[a] = [rec_x, rec_y]\n",
    "        a += 1\n",
    "\n",
    "    points_to_search = 100\n",
    "    with h5py.File(gradient, 'r+') as grad:\n",
    "        grad_points = grad['ELASTIC/coordinates'][:]\n",
    "        grad_data = grad['ELASTIC/data'][:]\n",
    "    \n",
    "        gll_points = 25\n",
    "        for receiv in receivers:\n",
    "            for i in range(gll_points):\n",
    "                point_tree = cKDTree(grad_points[:, i, :])\n",
    "\n",
    "                idx = point_tree.query_ball_point(receiv, radius_in_km * 1000.0)\n",
    "                grad_data[0,idx,:,i] = 0.0\n",
    "                grad['ELASTIC/data'][:] = grad_data\n",
    "\n",
    "def cut_source_from_gradient(source, iteration, radius_in_km, copy=False):\n",
    "    # Cut the source region from gradient.\n",
    "    # Basically the same as the receiver cutting function except now we use a larger radius.\n",
    "    gradient = os.path.join(paths.gradients, iteration, source.name, \"gradient_gll.h5\")\n",
    "    orig = os.path.join(paths.gradients, iteration, source.name, \"original_gradient.h5\")\n",
    "    if copy:\n",
    "        if not os.path.exists(orig):\n",
    "            shutil.copy(gradient, orig)\n",
    "        else:\n",
    "            shutil.copy(orig, gradient)\n",
    "    src = [source.loc_x, source.loc_y]\n",
    "    points_to_search = 16\n",
    "    with h5py.File(gradient, 'r+') as grad:\n",
    "        grad_points = grad['ELASTIC/coordinates'][:]\n",
    "        grad_data = grad['ELASTIC/data'][:]\n",
    "    \n",
    "        gll_points = 25\n",
    "        for i in range(gll_points):\n",
    "            point_tree = cKDTree(grad_points[:, i, :])\n",
    "            idx = point_tree.query_ball_point(src, radius_in_km * 1000.0)\n",
    "            grad_data[0,idx,:,i] = 0.0\n",
    "        grad['ELASTIC/data'][:] = grad_data\n",
    "\n",
    "def cut_stuff_from_gradient(location, iteration, radius_in_km):\n",
    "    gradient = os.path.join(paths.gradients, iteration, source.name, \"gradient_gll.h5\")\n",
    "    orig = os.path.join(paths.gradients, iteration, source.name, \"original_gradient.h5\")\n",
    "    if not os.path.exists(orig):\n",
    "        shutil.copy(gradient, orig)\n",
    "    else:\n",
    "        shutil.copy(orig, gradient)\n",
    "    src = location\n",
    "    print(src)\n",
    "    points_to_search = 16\n",
    "    with h5py.File(gradient, 'r+') as grad:\n",
    "        grad_points = grad['ELASTIC/coordinates'][:]\n",
    "        grad_data = grad['ELASTIC/data'][:]\n",
    "    \n",
    "        gll_points = 25\n",
    "        for i in range(gll_points):\n",
    "            point_tree = cKDTree(grad_points[:, i, :])\n",
    "            idx = point_tree.query_ball_point(src, radius_in_km * 1000.0)\n",
    "            grad_data[0,idx,:,i] = 0.0\n",
    "        grad['ELASTIC/data'][:] = grad_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Salvus Opt Communication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import toml\n",
    "def move_gradient_for_salvus_opt(iteration):\n",
    "    salvusopt_dir = os.path.join(paths.root, \"SALVUSOPT\")\n",
    "    if not os.path.exists(salvusopt_dir):\n",
    "        os.mkdir(salvusopt_dir)\n",
    "    grad_name = \"gradient_\" + iteration + \".e\"\n",
    "    dest_gradient = os.path.join(salvusopt_dir, \"PHYSICAL_MODELS\", grad_name)\n",
    "    start_gradient = os.path.join(paths.gradients, iteration, \"smooth_model.e\")\n",
    "    shutil.copy(start_gradient, dest_gradient)\n",
    "\n",
    "def get_model_from_salvus_opt(iteration):\n",
    "    salvusopt_dir = os.path.join(paths.root, \"SALVUSOPT\")\n",
    "    if not os.path.exists(salvusopt_dir):\n",
    "        os.mkdir(salvusopt_dir)\n",
    "    model_name = iteration + \".e\"\n",
    "    dest_model = os.path.join(paths.meshes, iteration, \"cartesian_2d.e\")\n",
    "    start_model = os.path.join(salvusopt_dir, \"PHYSICAL_MODELS\", model_name)\n",
    "    shutil.copy(start_model, dest_model)\n",
    "\n",
    "def write_in_gradient_path_to_task_toml(iteration):\n",
    "    grad_name = os.path.join(paths.salvusopt, \"PHYSICAL_MODELS\", \"gradient_\" + iteration + \".e\")\n",
    "    toml_filename = os.path.join(paths.salvusopt, \"task.toml\")\n",
    "    task_dict = toml.load(toml_filename)\n",
    "    task_dict[\"task\"][0][\"output\"][\"gradient\"] = grad_name\n",
    "    with open(toml_filename, \"w\") as fh:\n",
    "        toml.dump(task_dict, fh)\n",
    "    \n",
    "def write_misfit_to_task_toml(iteration):\n",
    "    misfits_toml = os.path.join(paths.adjoint_sources, iteration, \"misfits.toml\")\n",
    "    misfits_dict = toml.load(misfits_toml)\n",
    "    toml_filename = os.path.join(paths.salvusopt, \"task.toml\")\n",
    "    misfit = misfits_dict[\"misfits\"][\"total\"][\"total_misfit\"]\n",
    "    task_dict = toml.load(toml_filename)\n",
    "    task_dict[\"task\"][0][\"output\"][\"misfit\"] = float(misfit)\n",
    "    with open(toml_filename, \"w\") as fh:\n",
    "        toml.dump(task_dict, fh)\n",
    "\n",
    "def close_salvus_opt_task():\n",
    "    toml_filename = os.path.join(paths.salvusopt, \"task.toml\")\n",
    "    task_dict = toml.load(toml_filename)\n",
    "    task_dict[\"task\"][0][\"status\"][\"open\"] = False\n",
    "    with open(toml_filename, \"w\") as fh:\n",
    "        toml.dump(task_dict, fh)\n",
    "\n",
    "def get_new_iteration_names(iteration):\n",
    "    toml_filename = os.path.join(paths.salvusopt, \"task.toml\")\n",
    "    task_dict = toml.load(toml_filename)\n",
    "    new_iter = task_dict[\"task\"][0][\"input\"][\"model\"].split(\"/\")[-1][:-2]\n",
    "    prev_iter = iteration\n",
    "    return prev_iter, new_iter\n",
    "\n",
    "def read_salvus_opt_task():\n",
    "    toml_filename = os.path.join(paths.salvusopt, \"task.toml\")\n",
    "    task_dict = toml.load(toml_filename)\n",
    "    task = task_dict[\"task\"][0][\"type\"]\n",
    "    return task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now lets create the project\n",
    "\n",
    "This should of course be commented out if project already exists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_name = \"inversion_0\"\n",
    "create_project(project_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make Notebook know in which project we want to work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = Path(project_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define a few simulations parameters\n",
    "Central frequency, starting time, ending time, time step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_params = Parameters(1/5.0, -32.0, 250.0, 0.05, 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add sources and receivers\n",
    "\n",
    "This can be done arbitrarily but be careful that the sources and receivers are not too close to the boundary so that the absorbing boundaries will not be problematic. Make sure to have the parameter src_list correct as it is used multiple times in the code. It should include the actual source objects, not just the names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Source\n",
    "\n",
    "src_00 = Source(\"src_00\", 400, 400, [1e18, 5e17, 1e16])\n",
    "src_01 = Source(\"src_01\", 400, 700, [5e17, 1e18, 1e17])\n",
    "src_02 = Source(\"src_02\", 400, 1000, [1e17, 1e17, 1e18])\n",
    "src_10 = Source(\"src_10\", 700, 400, [1e18, 1e18, 1e18])\n",
    "src_11 = Source(\"src_11\", 700, 700, [1e18, 1e18, 1e17])\n",
    "src_12 = Source(\"src_12\", 700, 1000, [5e17, 5e17, 5e17])\n",
    "src_20 = Source(\"src_20\", 1000, 400, [1e18, 1e17, 1e17])\n",
    "src_21 = Source(\"src_21\", 1000, 700, [1e17, 5e17, 1e18])\n",
    "src_22 = Source(\"src_22\", 1000, 1000, [1e18, 4e17, 4e17])\n",
    "\n",
    "src_list = [src_00, src_01, src_02, src_10, src_11, src_12, src_20, src_21, src_22]\n",
    "\n",
    "\n",
    "rec_loc_x = np.arange(300, 1200, 100)\n",
    "rec_loc_y = np.arange(300, 1200, 100)\n",
    "\n",
    "src_x = []\n",
    "src_y = []\n",
    "for src in src_list:\n",
    "\n",
    "    b = 0\n",
    "    for s in rec_loc_x:\n",
    "        if b < 10:\n",
    "            x_name = \"0\" + str(b)\n",
    "        else:\n",
    "            x_name = str(b)\n",
    "        i = 0\n",
    "        for k in rec_loc_y:\n",
    "            if np.sqrt((k - src.loc_y / 1000.0) ** 2 + (s - src.loc_x / 1000.0) ** 2) < 100:\n",
    "                i += 1\n",
    "                continue\n",
    "            if i < 10:\n",
    "                y_name = \"0\" + str(i)\n",
    "            else:\n",
    "                y_name = str(i)\n",
    "            name = x_name + y_name\n",
    "            src.add_receiver(\"RG\", name, s, k)\n",
    "            i += 1\n",
    "        b += 1\n",
    "    src_x.append(src.loc_x/1000)\n",
    "    src_y.append(src.loc_y/1000)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mesh generation\n",
    "\n",
    "Unfortunately this code can not be made public at this point in time but I have included some example meshes in the folder \\\"meshes\\\" These can be used to run this script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MPI_BIN=\"/Drive/Shared/Salvus/bin/mpirun\"\n",
    "MPI_BIN=\"/home/solvi/miniconda3/envs/lasif/bin/mpirun\"\n",
    "SALVUS_BIN=\"/Drive/Shared/Salvus/bin/SalvusCompute\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a new iteration\n",
    "\n",
    "From here we can start the first iteration. Create a new iteration and go from there.\n",
    "\n",
    "Once a new iteration is created we move the homogeneous smoothiesems to their respective directories. We interpolate the correct velocities on to them and then we are ready for forward simulations.\n",
    "\n",
    "We can then generate the correct input files for Salvus, we should not need any new receiver.toml we can just copy the previously existing files.\n",
    "\n",
    "We then run the forward simulations, create the adjoint sources, run the adjoint simulations and interpolate the back on to the cartesian master model.\n",
    "\n",
    "The model is then updated and we start the looping process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iteration_name = \"it0000\" \n",
    "create_iteration(iteration_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Move the generated meshes into the first iteration and create hdf5 versions of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for src in src_list:\n",
    "    found_mesh = os.path.join(paths.meshes, \"INITIAL\", src.name, \"smoothiesem_2d.e\")\n",
    "    dest_mesh = os.path.join(paths.meshes, iteration_name, src.name, \"smoothiesem_2d.e\")\n",
    "    shutil.copy(found_mesh, dest_mesh)\n",
    "run_file = make_gll_model_from_exodus(iteration_name, source_list=src_list, mesh_type=\"smoothiesem\")\n",
    "# run_file = make_gll_model_from_exodus(iteration_name, source_list=src_list, mesh_type=\"cartesian\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "! sh $run_file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate input files for new iteration\n",
    "\n",
    "Generate the files that salvus needs in order to run a forward simulation on the smoothiesem meshes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for source in src_list:\n",
    "    generate_input_files(source=source, iteration=iteration_name, sim_params=sim_params, sim_type=\"forward\")\n",
    "    print(source.name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run a forward simulations for all the sources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_command = create_runfile(iteration_name, src_list, \"forward\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! $run_command"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Put synthetics in its correct place\n",
    "\n",
    "Get it ready for adjoint source calculations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for src in src_list:\n",
    "    retrieve_synthetics(source_name=src.name, iteration=iteration_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate adjoint sources at receiver positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "m = 0\n",
    "\n",
    "for src in src_list:\n",
    "    print(src.name)\n",
    "    \n",
    "    calculate_adjoint_sources(iteration=iteration_name, source=src, sim_params=sim_params)\n",
    "    \n",
    "    write_adjoint_source_to_file(iteration=iteration_name, source=src, sim_params=sim_params)\n",
    "    m += 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Write misfits to file so salvus opt can use them later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_misfit_to_file(iteration_name, src_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate input files for adjoint simulations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for src in src_list:\n",
    "    generate_input_files(source=src, iteration=iteration_name, sim_params=sim_params, sim_type=\"adjoint\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run adjoint simulations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_command = create_runfile(iteration_name, src_list, \"adjoint\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! $run_command"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Move Gradients to the correct folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for source in src_list:\n",
    "    retrieve_gradient(source.name, iteration_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make an empty cartesian mesh and sum gradients on top of it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cart_mesh = os.path.join(paths.gradients, \"kernel.e\")\n",
    "cart_dest = os.path.join(paths.gradients, iteration_name, \"cartesian_kernel.e\")\n",
    "shutil.copy(cart_mesh, cart_dest)\n",
    "\n",
    "for source in src_list:\n",
    "    cut_receiver_from_gradient(source, iteration_name, 5.0)\n",
    "    cut_source_from_gradient(source, iteration_name, 30.0, copy=False)\n",
    "interpolate_gradient_to_cartesian(iteration_name, src_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Smooth summed gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distribute_smoothers(iteration_name)\n",
    "run_smoother = smooth_gradient(iteration_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! $run_smoother"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "move_gradient_for_salvus_opt(iteration_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_in_gradient_path_to_task_toml(iteration_name)\n",
    "write_misfit_to_task_toml(iteration_name)\n",
    "close_salvus_opt_task()\n",
    "run_file = run_salvus_opt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! $run_file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Looping region\n",
    "\n",
    "The shell below can perform an arbitrary number of iterations automatically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import time\n",
    "max_it = 30\n",
    "rejected = 0\n",
    "simulation_time = 0\n",
    "adjoint_source_time = 0\n",
    "interpolation_time = 0\n",
    "cutting_time = 0\n",
    "for i in range(max_it):\n",
    "    print(\"==================================================================== \\n\")\n",
    "    print(str(i) + \"\\n\")\n",
    "    print(\"==================================================================== \\n\")\n",
    "    \n",
    "    # Name of an iteration which has the correct shape of meshes\n",
    "    iteration = \"it0000\"\n",
    "    task = read_salvus_opt_task()\n",
    "    if task != \"compute_misfit\":\n",
    "        print(\"SOMETHING WRONG\")\n",
    "        break\n",
    "    previous_iteration, iteration = get_new_iteration_names(iteration)\n",
    "    # # Create Iteration\n",
    "    \n",
    "    create_iteration(iteration)\n",
    "    \n",
    "    print(\"==================================================================== \\n \\n\")\n",
    "    print(f\"Current iteration: {iteration} \\n \\n\")\n",
    "    print(\"==================================================================== \\n\")\n",
    "    get_model_from_salvus_opt(iteration)\n",
    "    run_file = make_gll_model_from_exodus(iteration, mesh_type=\"cartesian\")\n",
    "    ! sh $run_file\n",
    "\n",
    "    # Copy meshes to the correct places\n",
    "    for src in src_list:\n",
    "        move_meshes_between_iterations(previous_iteration, iteration, src.name)\n",
    "\n",
    "    # Interpolate velocities to the meshes\n",
    "    start = time.time()\n",
    "    for src in src_list:\n",
    "        interpolate_from_cartesian_to_smoothie(iteration, src.name)\n",
    "    end = time.time()\n",
    "    interpolation_time += (end - start)\n",
    "\n",
    "    # Generate input files forward\n",
    "    for src in src_list:\n",
    "        generate_input_files(src, iteration, \"forward\", sim_params)\n",
    "\n",
    "    # Run forward simulations\n",
    "    start = time.time()\n",
    "    run_command = create_runfile(iteration, src_list, \"forward\")\n",
    "    ! $run_command\n",
    "    end = time.time()\n",
    "    simulation_time += (end - start)\n",
    "\n",
    "    # Retrieve synthetics\n",
    "    print(f\"Iteration: {iteration}\")\n",
    "    for src in src_list:\n",
    "        retrieve_synthetics(src.name, iteration)\n",
    "\n",
    "    # Generate Adjoint sources\n",
    "    start = time.time()\n",
    "    for src in src_list:\n",
    "        print(f\"Calculating adjoint sources for: {src.name}\")\n",
    "        calculate_adjoint_sources(iteration=iteration, source=src, sim_params=sim_params)\n",
    "        # Write them to file\n",
    "        write_adjoint_source_to_file(iteration=iteration, source=src, sim_params=sim_params)  \n",
    "    end = time.time()\n",
    "    adjoint_source_time += (end - start)\n",
    "    write_misfit_to_file(iteration, src_list)\n",
    "\n",
    "\n",
    "    write_misfit_to_task_toml(iteration)\n",
    "    close_salvus_opt_task()\n",
    "    run_file = run_salvus_opt()    \n",
    "\n",
    "    ! $run_file\n",
    "\n",
    "    task = read_salvus_opt_task()\n",
    "    if task != \"compute_gradient\":\n",
    "        print(\"==================================================================== \\n \\n\")\n",
    "        print(\"                         MODEL REJECTED \\n \\n\")\n",
    "        print(\"==================================================================== \\n\")\n",
    "        rejected += 1\n",
    "        continue\n",
    "    # Generate input files adjoint\n",
    "    for src in src_list:\n",
    "        generate_input_files(src, iteration, \"adjoint\", sim_params)\n",
    "\n",
    "    # Run adjoint simulations\n",
    "    start = time.time()\n",
    "    run_adjoint = create_runfile(iteration, src_list, \"adjoint\")\n",
    "    ! $run_adjoint\n",
    "    end = time.time()\n",
    "    simulation_time += (end - start)\n",
    "\n",
    "    # Retrieve gradients\n",
    "    for src in src_list:\n",
    "        retrieve_gradient(src.name, iteration)\n",
    "    cart_mesh = os.path.join(paths.gradients, \"kernel.e\")\n",
    "    cart_dest = os.path.join(paths.gradients, iteration, \"cartesian_kernel.e\")\n",
    "    shutil.copy(cart_mesh, cart_dest)\n",
    "\n",
    "    start = time.time()\n",
    "    for source in src_list:\n",
    "        cut_receiver_from_gradient(source, iteration, 5.0)\n",
    "        cut_source_from_gradient(source, iteration, 30.0, copy=False)\n",
    "    end = time.time()\n",
    "    cutting_time += (end - start)\n",
    "    \n",
    "    # sum all the smoothie source gradients on top of it\n",
    "    start = time.time()\n",
    "    interpolate_gradient_to_cartesian(iteration, src_list)\n",
    "    end = time.time()\n",
    "    interpolation_time += (end - start)\n",
    "    \n",
    "    # Smooth gradients\n",
    "    distribute_smoothers(iteration)\n",
    "    run_smoother = smooth_gradient(iteration)\n",
    "    ! $run_smoother\n",
    "\n",
    "    move_gradient_for_salvus_opt(iteration)\n",
    "\n",
    "    write_in_gradient_path_to_task_toml(iteration)\n",
    "    close_salvus_opt_task()\n",
    "    run_file = run_salvus_opt()\n",
    "    ! $run_file\n",
    "\n",
    "    close_salvus_opt_task()\n",
    "    run_file = run_salvus_opt()\n",
    "    ! $run_file\n",
    "print(f\"Model got rejected {rejected} times\")\n",
    "print(f\"Total simulation time: {simulation_time}\")\n",
    "print(f\"Total adjoint source calculation time: {adjoint_source_time}\")\n",
    "print(f\"Total interpolation time: {interpolation_time}\")\n",
    "print(f\"Total source/receiver cutting time: {cutting_time}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (new_lasif)",
   "language": "python",
   "name": "new_lasif"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
